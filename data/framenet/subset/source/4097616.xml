<?xml version="1.0" encoding="utf-8"?>
<command id="4097616">
	<sentence>Analytic tools include examining statements and articles in the news media , government reports including unclassified intelligence estimates , interviewing insiders who have had access to information about the clandestine program , reviewing secondary sources such as reports by analysts , and making scientific calculations based on the best available information about a country 's nuclear infrastructure .</sentence>
	<tokens>
		<token id="1" lemma="Analytic" pos="jj" surface="Analytic"/>
		<token id="2" lemma="tools" pos="nns" surface="tools"/>
		<token id="3" lemma="include" pos="VVP" surface="include"/>
		<token id="4" lemma="examining" pos="VVG" surface="examining"/>
		<token id="5" lemma="statements" pos="nns" surface="statements"/>
		<token id="6" lemma="and" pos="cc" surface="and"/>
		<token id="7" lemma="articles" pos="nns" surface="articles"/>
		<token id="8" lemma="in" pos="in" surface="in"/>
		<token id="9" lemma="the" pos="dt" surface="the"/>
		<token id="10" lemma="news" pos="nn" surface="news"/>
		<token id="11" lemma="media" pos="nns" surface="media"/>
		<token id="12" lemma="," pos="," surface=","/>
		<token id="13" lemma="government" pos="nn" surface="government"/>
		<token id="14" lemma="reports" pos="nns" surface="reports"/>
		<token id="15" lemma="including" pos="VVG" surface="including"/>
		<token id="16" lemma="unclassified" pos="jj" surface="unclassified"/>
		<token id="17" lemma="intelligence" pos="nn" surface="intelligence"/>
		<token id="18" lemma="estimates" pos="nns" surface="estimates"/>
		<token id="19" lemma="," pos="," surface=","/>
		<token id="20" lemma="interviewing" pos="VVG" surface="interviewing"/>
		<token id="21" lemma="insiders" pos="nns" surface="insiders"/>
		<token id="22" lemma="who" pos="wp" surface="who"/>
		<token id="23" lemma="have" pos="VHP" surface="have"/>
		<token id="24" lemma="had" pos="VHN" surface="had"/>
		<token id="25" lemma="access" pos="nn" surface="access"/>
		<token id="26" lemma="to" pos="to" surface="to"/>
		<token id="27" lemma="information" pos="nn" surface="information"/>
		<token id="28" lemma="about" pos="in" surface="about"/>
		<token id="29" lemma="the" pos="dt" surface="the"/>
		<token id="30" lemma="clandestine" pos="jj" surface="clandestine"/>
		<token id="31" lemma="program" pos="nn" surface="program"/>
		<token id="32" lemma="," pos="," surface=","/>
		<token id="33" lemma="reviewing" pos="VVG" surface="reviewing"/>
		<token id="34" lemma="secondary" pos="jj" surface="secondary"/>
		<token id="35" lemma="sources" pos="nns" surface="sources"/>
		<token id="36" lemma="such" pos="jj" surface="such"/>
		<token id="37" lemma="as" pos="in" surface="as"/>
		<token id="38" lemma="reports" pos="nns" surface="reports"/>
		<token id="39" lemma="by" pos="in" surface="by"/>
		<token id="40" lemma="analysts" pos="nns" surface="analysts"/>
		<token id="41" lemma="," pos="," surface=","/>
		<token id="42" lemma="and" pos="cc" surface="and"/>
		<token id="43" lemma="making" pos="VVG" surface="making"/>
		<token id="44" lemma="scientific" pos="jj" surface="scientific"/>
		<token id="45" lemma="calculations" pos="nns" surface="calculations"/>
		<token id="46" lemma="based" pos="VVN" surface="based"/>
		<token id="47" lemma="on" pos="in" surface="on"/>
		<token id="48" lemma="the" pos="dt" surface="the"/>
		<token id="49" lemma="best" pos="rbs" surface="best"/>
		<token id="50" lemma="available" pos="jj" surface="available"/>
		<token id="51" lemma="information" pos="nn" surface="information"/>
		<token id="52" lemma="about" pos="in" surface="about"/>
		<token id="53" lemma="a" pos="dt" surface="a"/>
		<token id="54" lemma="country" pos="nn" surface="country"/>
		<token id="55" lemma="'s" pos="POS" surface="'s"/>
		<token id="56" lemma="nuclear" pos="jj" surface="nuclear"/>
		<token id="57" lemma="infrastructure" pos="nn" surface="infrastructure"/>
		<token id="58" lemma="." pos="sent" surface="."/>
	</tokens>
	<semantics>
		<frameSemantics>
			<frame name="Searching">
				<lexicalUnit>
					<token id="40"/>
				</lexicalUnit>
				<frameElement type="Cognizer">
					<token id="40"/>
				</frameElement>
			</frame>
			<frame name="Searching">
				<lexicalUnit>
					<token id="1"/>
				</lexicalUnit>
				<frameElement type="Instrument">
					<token id="2"/>
				</frameElement>
			</frame>
			<frame name="Inspecting">
				<lexicalUnit>
					<token id="4"/>
				</lexicalUnit>
				<frameElement type="Ground">
					<token id="5"/>
					<token id="6"/>
					<token id="7"/>
					<token id="8"/>
					<token id="9"/>
					<token id="10"/>
					<token id="11"/>
				</frameElement>
			</frame>
		</frameSemantics>
	</semantics>
</command>
