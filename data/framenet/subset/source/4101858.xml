<?xml version="1.0" encoding="utf-8"?>
<command id="4101858">
	<sentence>One expert , whose job is so politically sensitive that he spoke on condition that he would n't be named or quoted , said the expected influx of East European refugees over the next few years will greatly increase the chances of computer-maintenance workers , for example , doubling as foreign spies .</sentence>
	<tokens>
		<token id="1" lemma="One" pos="cd" surface="One"/>
		<token id="2" lemma="expert" pos="nn" surface="expert"/>
		<token id="3" lemma="," pos="," surface=","/>
		<token id="4" lemma="whose" pos="wp$" surface="whose"/>
		<token id="5" lemma="job" pos="nn" surface="job"/>
		<token id="6" lemma="is" pos="VBZ" surface="is"/>
		<token id="7" lemma="so" pos="rb" surface="so"/>
		<token id="8" lemma="politically" pos="rb" surface="politically"/>
		<token id="9" lemma="sensitive" pos="jj" surface="sensitive"/>
		<token id="10" lemma="that" pos="in" surface="that"/>
		<token id="11" lemma="he" pos="PP" surface="he"/>
		<token id="12" lemma="spoke" pos="VVD" surface="spoke"/>
		<token id="13" lemma="on" pos="in" surface="on"/>
		<token id="14" lemma="condition" pos="nn" surface="condition"/>
		<token id="15" lemma="that" pos="in" surface="that"/>
		<token id="16" lemma="he" pos="PP" surface="he"/>
		<token id="17" lemma="would" pos="md" surface="would"/>
		<token id="18" lemma="n't" pos="rb" surface="n't"/>
		<token id="19" lemma="be" pos="vb" surface="be"/>
		<token id="20" lemma="named" pos="VVN" surface="named"/>
		<token id="21" lemma="or" pos="cc" surface="or"/>
		<token id="22" lemma="quoted" pos="VVN" surface="quoted"/>
		<token id="23" lemma="," pos="," surface=","/>
		<token id="24" lemma="said" pos="VVD" surface="said"/>
		<token id="25" lemma="the" pos="dt" surface="the"/>
		<token id="26" lemma="expected" pos="VVN" surface="expected"/>
		<token id="27" lemma="influx" pos="nn" surface="influx"/>
		<token id="28" lemma="of" pos="in" surface="of"/>
		<token id="29" lemma="East" pos="jj" surface="East"/>
		<token id="30" lemma="European" pos="jj" surface="European"/>
		<token id="31" lemma="refugees" pos="nns" surface="refugees"/>
		<token id="32" lemma="over" pos="in" surface="over"/>
		<token id="33" lemma="the" pos="dt" surface="the"/>
		<token id="34" lemma="next" pos="jj" surface="next"/>
		<token id="35" lemma="few" pos="jj" surface="few"/>
		<token id="36" lemma="years" pos="nns" surface="years"/>
		<token id="37" lemma="will" pos="md" surface="will"/>
		<token id="38" lemma="greatly" pos="rb" surface="greatly"/>
		<token id="39" lemma="increase" pos="VV" surface="increase"/>
		<token id="40" lemma="the" pos="dt" surface="the"/>
		<token id="41" lemma="chances" pos="nns" surface="chances"/>
		<token id="42" lemma="of" pos="in" surface="of"/>
		<token id="43" lemma="computer-maintenance" pos="jj" surface="computer-maintenance"/>
		<token id="44" lemma="workers" pos="nns" surface="workers"/>
		<token id="45" lemma="," pos="," surface=","/>
		<token id="46" lemma="for" pos="in" surface="for"/>
		<token id="47" lemma="example" pos="nn" surface="example"/>
		<token id="48" lemma="," pos="," surface=","/>
		<token id="49" lemma="doubling" pos="VVG" surface="doubling"/>
		<token id="50" lemma="as" pos="in" surface="as"/>
		<token id="51" lemma="foreign" pos="jj" surface="foreign"/>
		<token id="52" lemma="spies" pos="nns" surface="spies"/>
		<token id="53" lemma="." pos="sent" surface="."/>
	</tokens>
	<semantics>
		<frameSemantics>
			<frame name="Entering">
				<lexicalUnit>
					<token id="27"/>
				</lexicalUnit>
				<frameElement type="Theme">
					<token id="28"/>
					<token id="29"/>
					<token id="30"/>
					<token id="31"/>
				</frameElement>
				<frameElement type="Time">
					<token id="32"/>
					<token id="33"/>
					<token id="34"/>
					<token id="35"/>
					<token id="36"/>
				</frameElement>
			</frame>
		</frameSemantics>
	</semantics>
</command>
