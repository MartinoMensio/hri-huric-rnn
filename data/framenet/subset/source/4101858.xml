<?xml version="1.0" encoding="utf-8"?>
<command id="4101858">
	<sentence>One expert , whose job is so politically sensitive that he spoke on condition that he would n't be named or quoted , said the expected influx of East European refugees over the next few years will greatly increase the chances of computer-maintenance workers , for example , doubling as foreign spies .</sentence>
	<tokens>
		<token id="1" lemma="one" pos="CD" surface="One"/>
		<token id="2" lemma="expert" pos="NN" surface="expert"/>
		<token id="3" lemma="," pos="," surface=","/>
		<token id="4" lemma="whose" pos="WP$" surface="whose"/>
		<token id="5" lemma="job" pos="NN" surface="job"/>
		<token id="6" lemma="be" pos="VBZ" surface="is"/>
		<token id="7" lemma="so" pos="RB" surface="so"/>
		<token id="8" lemma="politically" pos="RB" surface="politically"/>
		<token id="9" lemma="sensitive" pos="JJ" surface="sensitive"/>
		<token id="10" lemma="that" pos="IN" surface="that"/>
		<token id="11" lemma="-PRON-" pos="PRP" surface="he"/>
		<token id="12" lemma="speak" pos="VBD" surface="spoke"/>
		<token id="13" lemma="on" pos="IN" surface="on"/>
		<token id="14" lemma="condition" pos="NN" surface="condition"/>
		<token id="15" lemma="that" pos="IN" surface="that"/>
		<token id="16" lemma="-PRON-" pos="PRP" surface="he"/>
		<token id="17" lemma="would" pos="MD" surface="would"/>
		<token id="18" lemma="n't" pos="RB" surface="n't"/>
		<token id="19" lemma="be" pos="VB" surface="be"/>
		<token id="20" lemma="name" pos="VBN" surface="named"/>
		<token id="21" lemma="or" pos="CC" surface="or"/>
		<token id="22" lemma="quote" pos="VBN" surface="quoted"/>
		<token id="23" lemma="," pos="," surface=","/>
		<token id="24" lemma="say" pos="VBD" surface="said"/>
		<token id="25" lemma="the" pos="DT" surface="the"/>
		<token id="26" lemma="expected" pos="JJ" surface="expected"/>
		<token id="27" lemma="influx" pos="NN" surface="influx"/>
		<token id="28" lemma="of" pos="IN" surface="of"/>
		<token id="29" lemma="east" pos="JJ" surface="East"/>
		<token id="30" lemma="european" pos="JJ" surface="European"/>
		<token id="31" lemma="refugee" pos="NNS" surface="refugees"/>
		<token id="32" lemma="over" pos="IN" surface="over"/>
		<token id="33" lemma="the" pos="DT" surface="the"/>
		<token id="34" lemma="next" pos="JJ" surface="next"/>
		<token id="35" lemma="few" pos="JJ" surface="few"/>
		<token id="36" lemma="year" pos="NNS" surface="years"/>
		<token id="37" lemma="will" pos="MD" surface="will"/>
		<token id="38" lemma="greatly" pos="RB" surface="greatly"/>
		<token id="39" lemma="increase" pos="VB" surface="increase"/>
		<token id="40" lemma="the" pos="DT" surface="the"/>
		<token id="41" lemma="chance" pos="NNS" surface="chances"/>
		<token id="42" lemma="of" pos="IN" surface="of"/>
		<token id="43" lemma="computer-maintenance" pos="JJ" surface="computer-maintenance"/>
		<token id="44" lemma="worker" pos="NNS" surface="workers"/>
		<token id="45" lemma="," pos="," surface=","/>
		<token id="46" lemma="for" pos="IN" surface="for"/>
		<token id="47" lemma="example" pos="NN" surface="example"/>
		<token id="48" lemma="," pos="," surface=","/>
		<token id="49" lemma="double" pos="VBG" surface="doubling"/>
		<token id="50" lemma="as" pos="IN" surface="as"/>
		<token id="51" lemma="foreign" pos="JJ" surface="foreign"/>
		<token id="52" lemma="spy" pos="NNS" surface="spies"/>
		<token id="53" lemma="." pos="." surface="."/>
	</tokens>
	<dependencies>
		<dep from="2" to="1" type="nummod"/>
		<dep from="24" to="2" type="nsubj"/>
		<dep from="2" to="3" type="punct"/>
		<dep from="5" to="4" type="poss"/>
		<dep from="6" to="5" type="nsubj"/>
		<dep from="2" to="6" type="relcl"/>
		<dep from="9" to="7" type="advmod"/>
		<dep from="9" to="8" type="advmod"/>
		<dep from="6" to="9" type="acomp"/>
		<dep from="12" to="10" type="mark"/>
		<dep from="12" to="11" type="nsubj"/>
		<dep from="9" to="12" type="ccomp"/>
		<dep from="12" to="13" type="prep"/>
		<dep from="13" to="14" type="pobj"/>
		<dep from="20" to="15" type="mark"/>
		<dep from="20" to="16" type="nsubjpass"/>
		<dep from="20" to="17" type="aux"/>
		<dep from="20" to="18" type="neg"/>
		<dep from="20" to="19" type="auxpass"/>
		<dep from="14" to="20" type="acl"/>
		<dep from="20" to="21" type="cc"/>
		<dep from="20" to="22" type="conj"/>
		<dep from="2" to="23" type="punct"/>
		<dep from="0" to="24" type="root"/>
		<dep from="27" to="25" type="det"/>
		<dep from="27" to="26" type="amod"/>
		<dep from="39" to="27" type="nsubj"/>
		<dep from="27" to="28" type="prep"/>
		<dep from="30" to="29" type="amod"/>
		<dep from="31" to="30" type="amod"/>
		<dep from="28" to="31" type="pobj"/>
		<dep from="27" to="32" type="prep"/>
		<dep from="36" to="33" type="det"/>
		<dep from="36" to="34" type="amod"/>
		<dep from="36" to="35" type="amod"/>
		<dep from="32" to="36" type="pobj"/>
		<dep from="39" to="37" type="aux"/>
		<dep from="39" to="38" type="advmod"/>
		<dep from="24" to="39" type="ccomp"/>
		<dep from="41" to="40" type="det"/>
		<dep from="39" to="41" type="dobj"/>
		<dep from="41" to="42" type="prep"/>
		<dep from="44" to="43" type="amod"/>
		<dep from="42" to="44" type="pobj"/>
		<dep from="39" to="45" type="punct"/>
		<dep from="39" to="46" type="prep"/>
		<dep from="46" to="47" type="pobj"/>
		<dep from="39" to="48" type="punct"/>
		<dep from="39" to="49" type="advcl"/>
		<dep from="49" to="50" type="prep"/>
		<dep from="52" to="51" type="amod"/>
		<dep from="50" to="52" type="pobj"/>
		<dep from="24" to="53" type="punct"/>
	</dependencies>
	<semantics>
		<frameSemantics>
			<frame name="Entering">
				<lexicalUnit>
					<token id="27"/>
				</lexicalUnit>
				<frameElement type="Theme">
					<token id="28"/>
					<token id="29"/>
					<token id="30"/>
					<token id="31"/>
				</frameElement>
				<frameElement type="Time">
					<token id="32"/>
					<token id="33"/>
					<token id="34"/>
					<token id="35"/>
					<token id="36"/>
				</frameElement>
			</frame>
		</frameSemantics>
	</semantics>
</command>
