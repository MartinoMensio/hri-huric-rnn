<?xml version="1.0" encoding="utf-8"?>
<command id="4097613">
	<sentence>Nonetheless , countries suspected of having clandestine nuclear weapons programs are the subject of intense scrutiny by nonproliferation analysts , intelligence agencies , and other observers .</sentence>
	<tokens>
		<token id="1" lemma="Nonetheless" pos="rb" surface="Nonetheless"/>
		<token id="2" lemma="," pos="," surface=","/>
		<token id="3" lemma="countries" pos="nns" surface="countries"/>
		<token id="4" lemma="suspected" pos="VVD" surface="suspected"/>
		<token id="5" lemma="of" pos="in" surface="of"/>
		<token id="6" lemma="having" pos="VHG" surface="having"/>
		<token id="7" lemma="clandestine" pos="jj" surface="clandestine"/>
		<token id="8" lemma="nuclear" pos="jj" surface="nuclear"/>
		<token id="9" lemma="weapons" pos="nns" surface="weapons"/>
		<token id="10" lemma="programs" pos="nns" surface="programs"/>
		<token id="11" lemma="are" pos="vbp" surface="are"/>
		<token id="12" lemma="the" pos="dt" surface="the"/>
		<token id="13" lemma="subject" pos="nn" surface="subject"/>
		<token id="14" lemma="of" pos="in" surface="of"/>
		<token id="15" lemma="intense" pos="jj" surface="intense"/>
		<token id="16" lemma="scrutiny" pos="nn" surface="scrutiny"/>
		<token id="17" lemma="by" pos="in" surface="by"/>
		<token id="18" lemma="nonproliferation" pos="nn" surface="nonproliferation"/>
		<token id="19" lemma="analysts" pos="nns" surface="analysts"/>
		<token id="20" lemma="," pos="," surface=","/>
		<token id="21" lemma="intelligence" pos="nn" surface="intelligence"/>
		<token id="22" lemma="agencies" pos="nns" surface="agencies"/>
		<token id="23" lemma="," pos="," surface=","/>
		<token id="24" lemma="and" pos="cc" surface="and"/>
		<token id="25" lemma="other" pos="jj" surface="other"/>
		<token id="26" lemma="observers" pos="nns" surface="observers"/>
		<token id="27" lemma="." pos="sent" surface="."/>
	</tokens>
	<semantics>
		<frameSemantics>
			<frame name="Searching">
				<lexicalUnit>
					<token id="16"/>
				</lexicalUnit>
				<frameElement type="Cognizer">
					<token id="17"/>
					<token id="18"/>
					<token id="19"/>
					<token id="20"/>
					<token id="21"/>
					<token id="22"/>
					<token id="23"/>
					<token id="24"/>
					<token id="25"/>
					<token id="26"/>
				</frameElement>
			</frame>
			<frame name="Searching">
				<lexicalUnit>
					<token id="19"/>
				</lexicalUnit>
				<frameElement type="Cognizer">
					<token id="19"/>
				</frameElement>
				<frameElement type="Phenomenon">
					<token id="18"/>
				</frameElement>
			</frame>
		</frameSemantics>
	</semantics>
</command>
