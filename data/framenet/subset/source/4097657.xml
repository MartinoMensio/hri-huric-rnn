<?xml version="1.0" encoding="utf-8"?>
<command id="4097657">
	<sentence>The IAEA 's ability to directly monitor activities at the Yo ( ngbyo ( n nuclear complex was completely lost in late December 2002 when North Korea expelled IAEA inspectors who had been monitoring the freeze .</sentence>
	<tokens>
		<token id="1" lemma="The" pos="dt" surface="The"/>
		<token id="2" lemma="IAEA" pos="nn" surface="IAEA"/>
		<token id="3" lemma="'s" pos="POS" surface="'s"/>
		<token id="4" lemma="ability" pos="nn" surface="ability"/>
		<token id="5" lemma="to" pos="to" surface="to"/>
		<token id="6" lemma="directly" pos="rb" surface="directly"/>
		<token id="7" lemma="monitor" pos="VV" surface="monitor"/>
		<token id="8" lemma="activities" pos="nns" surface="activities"/>
		<token id="9" lemma="at" pos="in" surface="at"/>
		<token id="10" lemma="the" pos="dt" surface="the"/>
		<token id="11" lemma="Yo" pos="NP" surface="Yo"/>
		<token id="12" lemma="(" pos="(" surface="("/>
		<token id="13" lemma="ngbyo" pos="nn" surface="ngbyo"/>
		<token id="14" lemma="(" pos="(" surface="("/>
		<token id="15" lemma="n" pos="nn" surface="n"/>
		<token id="16" lemma="nuclear" pos="jj" surface="nuclear"/>
		<token id="17" lemma="complex" pos="nn" surface="complex"/>
		<token id="18" lemma="was" pos="VBD" surface="was"/>
		<token id="19" lemma="completely" pos="rb" surface="completely"/>
		<token id="20" lemma="lost" pos="VVN" surface="lost"/>
		<token id="21" lemma="in" pos="in" surface="in"/>
		<token id="22" lemma="late" pos="jj" surface="late"/>
		<token id="23" lemma="December" pos="NP" surface="December"/>
		<token id="24" lemma="2002" pos="cd" surface="2002"/>
		<token id="25" lemma="when" pos="wrb" surface="when"/>
		<token id="26" lemma="North" pos="NP" surface="North"/>
		<token id="27" lemma="Korea" pos="NP" surface="Korea"/>
		<token id="28" lemma="expelled" pos="VVD" surface="expelled"/>
		<token id="29" lemma="IAEA" pos="nn" surface="IAEA"/>
		<token id="30" lemma="inspectors" pos="nns" surface="inspectors"/>
		<token id="31" lemma="who" pos="wp" surface="who"/>
		<token id="32" lemma="had" pos="VHD" surface="had"/>
		<token id="33" lemma="been" pos="VBN" surface="been"/>
		<token id="34" lemma="monitoring" pos="VVG" surface="monitoring"/>
		<token id="35" lemma="the" pos="dt" surface="the"/>
		<token id="36" lemma="freeze" pos="nn" surface="freeze"/>
		<token id="37" lemma="." pos="sent" surface="."/>
	</tokens>
	<semantics>
		<frameSemantics>
			<frame name="Searching">
				<lexicalUnit>
					<token id="7"/>
				</lexicalUnit>
				<frameElement type="Ground">
					<token id="8"/>
					<token id="9"/>
					<token id="10"/>
					<token id="11"/>
					<token id="12"/>
					<token id="13"/>
					<token id="14"/>
					<token id="15"/>
					<token id="16"/>
					<token id="17"/>
				</frameElement>
				<frameElement type="Cognizer">
					<token id="1"/>
					<token id="2"/>
					<token id="3"/>
				</frameElement>
				<frameElement type="Manner">
					<token id="6"/>
				</frameElement>
			</frame>
			<frame name="Searching">
				<lexicalUnit>
					<token id="34"/>
				</lexicalUnit>
				<frameElement type="Ground">
					<token id="35"/>
					<token id="36"/>
				</frameElement>
				<frameElement type="Cognizer">
					<token id="31"/>
				</frameElement>
				<frameElement type="Cognizer">
					<token id="29"/>
					<token id="30"/>
				</frameElement>
			</frame>
			<frame name="Searching">
				<lexicalUnit>
					<token id="30"/>
				</lexicalUnit>
				<frameElement type="Cognizer">
					<token id="30"/>
				</frameElement>
			</frame>
		</frameSemantics>
	</semantics>
</command>
