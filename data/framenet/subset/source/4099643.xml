<?xml version="1.0" encoding="utf-8"?>
<command id="4099643">
	<sentence>The Department of Homeland Security ( DHS ) recently completed a study , conducted in response to a request I [ Carl Levin ] made with Senator Debbie Stabenow and Rep . John Dingell that bluntly acknowledges the security risk these trucks pose .</sentence>
	<tokens>
		<token id="1" lemma="The" pos="dt" surface="The"/>
		<token id="2" lemma="Department" pos="NP" surface="Department"/>
		<token id="3" lemma="of" pos="in" surface="of"/>
		<token id="4" lemma="Homeland" pos="nn" surface="Homeland"/>
		<token id="5" lemma="Security" pos="NP" surface="Security"/>
		<token id="6" lemma="(" pos="(" surface="("/>
		<token id="7" lemma="DHS" pos="NP" surface="DHS"/>
		<token id="8" lemma=")" pos=")" surface=")"/>
		<token id="9" lemma="recently" pos="rb" surface="recently"/>
		<token id="10" lemma="completed" pos="VVD" surface="completed"/>
		<token id="11" lemma="a" pos="dt" surface="a"/>
		<token id="12" lemma="study" pos="nn" surface="study"/>
		<token id="13" lemma="," pos="," surface=","/>
		<token id="14" lemma="conducted" pos="VVN" surface="conducted"/>
		<token id="15" lemma="in" pos="in" surface="in"/>
		<token id="16" lemma="response" pos="nn" surface="response"/>
		<token id="17" lemma="to" pos="to" surface="to"/>
		<token id="18" lemma="a" pos="dt" surface="a"/>
		<token id="19" lemma="request" pos="nn" surface="request"/>
		<token id="20" lemma="I" pos="PP" surface="I"/>
		<token id="21" lemma="[" pos="sym" surface="["/>
		<token id="22" lemma="Carl" pos="NP" surface="Carl"/>
		<token id="23" lemma="Levin" pos="NP" surface="Levin"/>
		<token id="24" lemma="]" pos="sym" surface="]"/>
		<token id="25" lemma="made" pos="VVN" surface="made"/>
		<token id="26" lemma="with" pos="in" surface="with"/>
		<token id="27" lemma="Senator" pos="NP" surface="Senator"/>
		<token id="28" lemma="Debbie" pos="NP" surface="Debbie"/>
		<token id="29" lemma="Stabenow" pos="NP" surface="Stabenow"/>
		<token id="30" lemma="and" pos="cc" surface="and"/>
		<token id="31" lemma="Rep" pos="NP" surface="Rep"/>
		<token id="32" lemma="." pos="sent" surface="."/>
		<token id="33" lemma="John" pos="NP" surface="John"/>
		<token id="34" lemma="Dingell" pos="NP" surface="Dingell"/>
		<token id="35" lemma="that" pos="wdt" surface="that"/>
		<token id="36" lemma="bluntly" pos="rb" surface="bluntly"/>
		<token id="37" lemma="acknowledges" pos="VVZ" surface="acknowledges"/>
		<token id="38" lemma="the" pos="dt" surface="the"/>
		<token id="39" lemma="security" pos="nn" surface="security"/>
		<token id="40" lemma="risk" pos="nn" surface="risk"/>
		<token id="41" lemma="these" pos="dt" surface="these"/>
		<token id="42" lemma="trucks" pos="nns" surface="trucks"/>
		<token id="43" lemma="pose" pos="VVP" surface="pose"/>
		<token id="44" lemma="." pos="sent" surface="."/>
	</tokens>
	<semantics>
		<frameSemantics>
			<frame name="Searching">
				<lexicalUnit>
					<token id="12"/>
				</lexicalUnit>
				<frameElement type="Cognizer">
					<token id="1"/>
					<token id="2"/>
					<token id="3"/>
					<token id="4"/>
					<token id="5"/>
					<token id="6"/>
					<token id="7"/>
					<token id="8"/>
				</frameElement>
				<frameElement type="Time">
					<token id="9"/>
				</frameElement>
			</frame>
		</frameSemantics>
	</semantics>
</command>
