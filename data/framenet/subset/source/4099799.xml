<?xml version="1.0" encoding="utf-8"?>
<command id="4099799">
	<sentence>They were there to study migrating birds , information the military would use to determine if chemicals and diseases could be transported across borders and so be used as vectors by -- or against -- an enemy .</sentence>
	<tokens>
		<token id="1" lemma="-PRON-" pos="PRP" surface="They"/>
		<token id="2" lemma="be" pos="VBD" surface="were"/>
		<token id="3" lemma="there" pos="RB" surface="there"/>
		<token id="4" lemma="to" pos="TO" surface="to"/>
		<token id="5" lemma="study" pos="VB" surface="study"/>
		<token id="6" lemma="migrate" pos="VBG" surface="migrating"/>
		<token id="7" lemma="bird" pos="NNS" surface="birds"/>
		<token id="8" lemma="," pos="," surface=","/>
		<token id="9" lemma="information" pos="NN" surface="information"/>
		<token id="10" lemma="the" pos="DT" surface="the"/>
		<token id="11" lemma="military" pos="NN" surface="military"/>
		<token id="12" lemma="would" pos="MD" surface="would"/>
		<token id="13" lemma="use" pos="VB" surface="use"/>
		<token id="14" lemma="to" pos="TO" surface="to"/>
		<token id="15" lemma="determine" pos="VB" surface="determine"/>
		<token id="16" lemma="if" pos="IN" surface="if"/>
		<token id="17" lemma="chemical" pos="NNS" surface="chemicals"/>
		<token id="18" lemma="and" pos="CC" surface="and"/>
		<token id="19" lemma="disease" pos="NNS" surface="diseases"/>
		<token id="20" lemma="could" pos="MD" surface="could"/>
		<token id="21" lemma="be" pos="VB" surface="be"/>
		<token id="22" lemma="transport" pos="VBN" surface="transported"/>
		<token id="23" lemma="across" pos="IN" surface="across"/>
		<token id="24" lemma="border" pos="NNS" surface="borders"/>
		<token id="25" lemma="and" pos="CC" surface="and"/>
		<token id="26" lemma="so" pos="RB" surface="so"/>
		<token id="27" lemma="be" pos="VB" surface="be"/>
		<token id="28" lemma="use" pos="VBN" surface="used"/>
		<token id="29" lemma="as" pos="IN" surface="as"/>
		<token id="30" lemma="vector" pos="NNS" surface="vectors"/>
		<token id="31" lemma="by" pos="RB" surface="by"/>
		<token id="32" lemma="--" pos=":" surface="--"/>
		<token id="33" lemma="or" pos="CC" surface="or"/>
		<token id="34" lemma="against" pos="IN" surface="against"/>
		<token id="35" lemma="--" pos=":" surface="--"/>
		<token id="36" lemma="an" pos="DT" surface="an"/>
		<token id="37" lemma="enemy" pos="NN" surface="enemy"/>
		<token id="38" lemma="." pos="." surface="."/>
	</tokens>
	<dependencies>
		<dep from="2" to="1" type="nsubj"/>
		<dep from="0" to="2" type="root"/>
		<dep from="2" to="3" type="advmod"/>
		<dep from="5" to="4" type="aux"/>
		<dep from="2" to="5" type="advcl"/>
		<dep from="7" to="6" type="amod"/>
		<dep from="5" to="7" type="dobj"/>
		<dep from="2" to="8" type="punct"/>
		<dep from="2" to="9" type="npadvmod"/>
		<dep from="11" to="10" type="det"/>
		<dep from="13" to="11" type="nsubj"/>
		<dep from="13" to="12" type="aux"/>
		<dep from="9" to="13" type="relcl"/>
		<dep from="15" to="14" type="aux"/>
		<dep from="13" to="15" type="xcomp"/>
		<dep from="22" to="16" type="mark"/>
		<dep from="22" to="17" type="nsubjpass"/>
		<dep from="17" to="18" type="cc"/>
		<dep from="17" to="19" type="conj"/>
		<dep from="22" to="20" type="aux"/>
		<dep from="22" to="21" type="auxpass"/>
		<dep from="15" to="22" type="ccomp"/>
		<dep from="22" to="23" type="prep"/>
		<dep from="23" to="24" type="pobj"/>
		<dep from="22" to="25" type="cc"/>
		<dep from="28" to="26" type="advmod"/>
		<dep from="28" to="27" type="auxpass"/>
		<dep from="22" to="28" type="conj"/>
		<dep from="28" to="29" type="prep"/>
		<dep from="29" to="30" type="pobj"/>
		<dep from="28" to="31" type="agent"/>
		<dep from="31" to="32" type="punct"/>
		<dep from="31" to="33" type="cc"/>
		<dep from="31" to="34" type="conj"/>
		<dep from="34" to="35" type="punct"/>
		<dep from="37" to="36" type="det"/>
		<dep from="34" to="37" type="pobj"/>
		<dep from="2" to="38" type="punct"/>
	</dependencies>
	<semantics>
		<frameSemantics>
			<frame name="Searching">
				<lexicalUnit>
					<token id="5"/>
				</lexicalUnit>
				<frameElement type="Cognizer">
					<token id="1"/>
				</frameElement>
				<frameElement type="Ground">
					<token id="6"/>
					<token id="7"/>
				</frameElement>
			</frame>
			<frame name="Bringing">
				<lexicalUnit>
					<token id="22"/>
				</lexicalUnit>
				<frameElement type="Theme">
					<token id="17"/>
					<token id="18"/>
					<token id="19"/>
				</frameElement>
				<frameElement type="Path">
					<token id="23"/>
					<token id="24"/>
				</frameElement>
			</frame>
		</frameSemantics>
	</semantics>
</command>
